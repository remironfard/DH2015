%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[conference]{IEEEtran}
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}


\usepackage[francais,english]{babel}
\usepackage{aeguill}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage[latin1]{inputenc} %% For ISO-latin1 chars
                                %% (accented letters).
\usepackage[hyphens]{url}
\usepackage[pdftex,urlcolor=black,colorlinks=true,linkcolor=black,citecolor=black]{hyperref}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Subsection}

\usepackage{txfonts}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )

% todo macro
\usepackage{color}
\newcommand{\todo}[1]{\noindent\textcolor{red}{{\bf \{ToDo} #1{\bf \}}}}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor ALIZE}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Video Recording and Indexing of Theater Rehearsals}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
\author{\IEEEauthorblockN{Rémi Ronfard,\IEEEauthorrefmark{1} Vincet \todo{Vincent ou Vincet} Gandhi,\IEEEauthorrefmark{1}
Benoit Encelle,\IEEEauthorrefmark{2} P.-A. Champin,\IEEEauthorrefmark{2} Thomas Steiner\IEEEauthorrefmark{2},
Nicolas Sauret,\IEEEauthorrefmark{3} 
Cyrille Migniot\IEEEauthorrefmark{4}}
\IEEEauthorblockA{\IEEEauthorrefmark{1} INRIA, LJK, University of Grenoble, France.
Email: remi.ronfard@inria.fr}
\IEEEauthorblockA{\IEEEauthorrefmark{2} CNRS, Université de Lyon. LIRIS, UMR5205. Université Lyon~1, France.\\
Email: \{bencelle, pierre-antoine.champin, tsteiner\}@liris.cnrs.fr}
\IEEEauthorblockA{\IEEEauthorrefmark{3} IRI, France.
Email: nicolas.sauret@iri.centrepompidou.fr}
\IEEEauthorblockA{\IEEEauthorrefmark{4} University of Burgundy, France.
Email: cyrille.migniot@u-bourgogne.fr}
}


% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}


% make the title area
\maketitle


\begin{abstract}
%\boldmath
\todo{Write abstract}
\end{abstract}
% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the conference you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals/conferences frown on
% math in the abstract anyway.I. INTRODUCTION (1 page)

% no keywords

\begin{keywords}
\todo{Write keywords}
\end{keywords}

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



%%%%%%%%%%%%%%%%
\section{Introduction}

\todo{0.5 pages (Rémi)}
\todo{BE : Remarque générale :-attention à l’aspect Opéra : pas présent pour l’instant au moins dans le titre, l’introduction et la partie état de l’art… S’il faut gagner de la place, il faudra peut-être faire sans (simplement en mentionnant dans le papier le fait que le processus a également été appliqué sur de l’opéra mais que pour des raisons de place…) }

Within the world of theater the rehearsal room is a sacred space---the private domain where boundaries are pushed, risks are taken, mistakes made, vulnerabilities exposed and, at its very best, magic created. It is not a place into which the public is often, if ever, invited. Until now. (reproduced from the liner notes to ``in the company of actors'').
\todo{BE : mettre en footnote la source non ?}
 This paper describes the result of a two-year project dedicated to the creation of a digital archive  of  the complete  rehearsals of a theater production at theater de Lyon Célestins.

\todo{Spectacle en Ligne(s): complete workflow for a natively digital archive from capture to editorialization
(captation/annotation/enrichissement/publication/éditorialisation)}
\todo{BE: quelle différence entre annotation et enrichissement ? --> je pense que seul "annotation" suffit)}

In 1966, Jean-Luc Godard remarked: {\em ``Why do theater people never film their performances to keep  them as an archive? This would be very simple: put the camera in the middle of the orchestra seats with a medium lens---not a zoom lens, because it would already be making choices and propose an interpretation''}~\cite{Godard66}.

The objective of the \emph{Spectacle en Ligne(s)} research project was to develop novel intuitive and interactive software tools to make it easier to 
create and publish annotated video archives of theater performance and rehearsals. As a case study, the project is releasing an exhaustive archive of the complete rehearsals of two performances played in 2013 : 1) a French translation and adaptation of \emph{Cat on a hot tin roof} written by Tennessee Williams and directed by Claudia Stavisky, 2) the baroque opera \emph{Elena}, composed by Francesco Cavalli, directed by Jean-Yves Ruf and conducted by Leonardo García Alarcón.

The French title of the project refers to the multiple timelines at work in this archive: all rehearsals timelines, from the first readings to the opening night, in addition to the storyline of the play. The title also refers to putting the video archive "online" (i.e. "en ligne") on the web.

\todo{Paragraph on legal aspects? NS> why not in the limitations instead} 

\todo{Paragraph on motivations and audiences?}

\todo{Paragraph on technical achievements of the project?}

The paper is organized as follows. Section~1 reviews previous work in using video as an archive for theater. Section~2 describes
context of theater rehearsals and introduces the key concepts that govern the archive. Section~3 describes the workflow that was
created to produce, annotate, and publish the archive. Section~4 describes usage scenarios and applications that were developed
to support access to the archive by different audiences. Section~5 reviews limitations and future work.
\todo{Make sure the paper structure is correctly described}


%%%%%%%%%%%%%%%%
\section{State of the art}
\todo{State of the art in theater, computers and video (1 page - Rémi)}

\todo{Why record rehearsals?}
\todo{Why index them?}
\todo{Who are the targeted audiences? Professors, researchers, students, and amateurs?}
\todo{We can use Pascal Bouchez evaluation grids to assess the quality of the dataset?}
\todo{Our common goal is to show theater at work?}

\todo{Not much work dedicated to document the rehearsal process.}

\todo{Not much work dedicated to document the mise-en-scene.}

\todo{Mention the Brecht archives, the Stanislavsky archives.}

\todo{[Regarder l'annexe technique du projet]}
\todo{[+ présentation REMI à la FFIRT?]}

Archiving the recordings of theater performances is important for preserving  cultural heritage. 
It is not yet a common or systematic practice, and theater remains known as {\em the ephemeral art}\todo{cite}.
This is likely to change in the near future, as high-quality video recordings are becoming more widely
available to theaters.   Many organizations around the world now actively produce and
archive video recordings of theater performances. The theater on Film and Tape Archive
(TOFT) provides research access to video recordings of New York and regional theater productions
since 1970 and the collection now amounts to 7,469 titles with a total of 20,000 items. 
The National Video Archive of Performance (NVAP) at Victoria and Albert Museum London
provides research access to video recordings of numerous renowned theater performances produced
across England since 1992. 

Another relevant case study  is the successful ventures providing recorded
theater performances on the Web. Recently launched companies like Digital Theater use their
own equipment to record theater productions, then sell them for far less than the cost of an
actual theater ticket to watch online, but from the best spot possible. The Website \todo{add footnote} of the French
National Institute of Audiovisual (INA) also allows viewers to download recorded theater plays
at considerably low prices. You can watch the trailers of theater performances just like movies
on a variety of devices and then download the entire play if it is of your interest. The Web platforms
have also allowed for free form experimentation with recorded sequences, which is not
so convenient on actual TV or cinema. For example, France Televisions' Nouvelles Ecritures\footnote{Nouvelles Ecritures is the new media departement at France Televisions producing and experimenting new format : webdoc, transmedia, virtual reality, \emph{etc.}: \url{http://nouvelles-ecritures.francetv.fr/}} :  created in 2013 an augmented theater experience for the web\footnote{Théâtre sans animaux : \url{http://nouvelles-ecritures.francetv.fr/theatre-sans-animaux/}} showing five different versions of the same play \emph{Théâtre sans animaux}\footnote{\emph{Théatre sans animaux} from Jean-Michel Ribes}, showing both how a theater play develops in different stages and how it can be presented to the audience in different styles. The Web platform has also allowed several theaters to share the extracts and trailers of upcoming performances for publicity purposes. Importantly, Web based platforms allow even the small production companies or individuals to easily share the recorded content with the desired audience.

\todo{NS pourquoi ce paragraph est dans "state of the art" ?}
In this paper, we describe our effort to create exhaustive video recordings of the complete rehearsals, rather than a single performance, of a theater play and publishing it online. This raises original and interesting scientific, technological, and social issues, and brought us to question the nature of the archive as a digital heritage through various prospective usages (see Section V \todo{verifier référence section}). 

\todo{i believe this paper talks about something else than multiplying the points of view @Remi: need to be reformulated : "our approach in this paper..."}
\paragraph*{Video recording strategy}
Our approach in this paper is to make video recordings from a single viewpoint (the director's seat) with the highest possible video resolution and to augment them with detailed, high-level  annotations that make it possible to quickly  search, browse and even re-edit the archive interactively. 

\todo{Show image of the camera and room?}

What we lose is the variety of viewpoints, which is typical for television broadcasts. What we gain is the ability to compare different versions, measure progress, and understand the rehearsal process by the virtue of having a single spatial reference.
We also experimented with novel techniques for varying the framings and shot sizes interactively 
by making use of a {\em virtual}  zoom lens~\cite{Gandhi14}, which makes it possible for the audience to
use the archive as {\em stock footage} which they can re-frame and re-edit according to their own interpretation (this is described in Section 5) \todo{Make sure this is correct}. 


\paragraph*{Video and theater}

The analysis of performance requires a working knowledge of semiotics, reception studies and theories 
of ideology, gender, corporeality, space and ethnicity in addition to experience of the art form in question~\cite{Auslander97,Counsell01}.

Audio and video recordings are useful resources for studying audio and visual interaction between actors~\cite{Fitzpatrick90}
and of course in the field of rehearsal studies~\cite{McAuley98a,McAuley98b,McAuley06,McAuley08}.

Yet, in practice, researchers have limited access to performances and must often refer to their own memories of the production or the rehearsals~\cite{Selbourne82,Sher85,Stafford00,Stern00}. A notable exception is {\em In the company of actors}, a documentary featuring an ensemble of Australian actors, as they prepare to perform the Sydney theater Company's production of \emph{Hedda Gabler}, with Cate Blanchett in the title role at the prestigious Brooklyn Academy of Music  in New York~\cite{Darling07}. But even in that case, only an edited version (an interpretation)  of the rehearsals is made available to the public. 

\todo{Other exceptions: Ariane Mnouchkine, John Gelgud.}

\paragraph*{Audio and video processing}
A useful feature for a large video archive is the capability to retrieve video segments based on which actors 
are performing and which segment of the play script they are performing. The topics of actor~\cite{Hilton06} and speaker~\cite{Miro12} recognition are very active in the audio and video processing research communities, but very little academic 
work has been dedicated to the special case of theater. The theater stage is a complex and cluttered environment 
with complex lighting and vast dimensions, which makes automatic audio and video processing quite challenging. 
Swedbeerg describes a system for localizing actors on a stage using RFID sensors carried by the actors~\cite{Swedberg}
but their system is intrusive and costly, which severely limits its usefulness in practice.

A common approach in computer vision for detecting and recognizing people uses generic people detectors~\cite{Ronfard02,
Dalal05,Felzenszwalb10,Andriluka12} followed by re-identification~\cite{TapaswiBS12}. Gandhi and Ronfard have evaluated 
that approach in the special cases of detecting and naming actors in films and on stage~\cite{Gandhi13} and found that such 
approaches miss the actors in 40\% of the cases on average. As an alternative, they have proposed
methods for separately learning appearance models of actors and detecting them in parallel, and showed that the number
of missed actors (false negatives) drops to 20\% on average. We used their method in \emph{Spectacle en Ligne(s)} for localizing
actors in selected scenes of the archive.

One approach to indexing of theater rehearsals and performances is to automatically align
them to the play script. The {\em script-sync} feature in Avid Media Composer,
an off-the-shelf tool, allows for the alignment of video recordings with their scripts word-by-word. Such systems
are based on Hidden Markov Models (HMM) and work by computing  a forced alignment 
between the play script and the audio recording. Nevertheless they require a very high quality 
sound recording and fail in the presence of music, reverberation and background noise. This makes them
unsuitable for our case. Speaker change detection (also called speaker diarization) is an alternative method
which provides line-by-line, rather than word-by-word alignments and has been demonstrated 
on television shows~\cite{Sankar09} and theater recordings~\cite{Caillet07,Caillet13}. In Section~5 \todo{Make sure this is correct},
we describe a special-purpose speaker diarization method that we have developed for aligning multiple versions 
of the same scene temporally using the common play script as a pivot representation.


\paragraph*{Hypervideo and interactive documentaries}

The term \emph{hypervideo} is commonly used to refer to
\textit{``a~displayed video stream that contains embedded user-clickable anchors''}%
~\cite{sawhney1996hypercafe,smith2002extensible}
and annotations, allowing for navigation between the video and other hypermedia elements.
In a~2006 article in \emph{The Economist}, the authors write 
\textit{``[h]yperlinking video involves the use of `object-tracking' software
to make filmed objects, such as cars, clickable as they move around.
Viewers can then click on items of interest in a~video
to watch a~related clip; after it has played,
the original video resumes where it left off.
To inform viewers that a~video is hyperlinked,
editors can add highlights to moving images, use beeps as audible cues,
or display still images from hyperlinked videos
next to the clip that is currently playing''}~\cite{economist2006hypervideo}.
In standard literature, hypervideo is considered a~logical consequence
of the related concept of \emph{hypertext}~\cite{bernerslee1990hypertext}.
In contrast to hypertext, hypervideo necessarily includes a~time component,
as content changes over time.
Therefore hypervideo has other technical and aesthetic requirements
than hypertext, the most obvious one being appropriate segmentation in scenes
or even objects.
The opportunities for feature-rich semantic hypervideos are endless,
only limited by feasibility and ease of their creation.
In this paper, we share our approach to affordably and practically document
the creation of theater and opera productions with video and Web technologies.


%%%%%%%%%%%%%%%%
\section{The rehearsal process context}

\todo{Contexte de l'archive (1 page - Nicolas)}

\todo{du terrain à la donnée: ce qui nous a amené à structurer les données de cette façon}

\todo{description des besoins}

\todo{Actors and participants:  theater rehearsals involve many actors and participants, not limited to actors in the play.}

\todo{Other participants include the director and her assistants, the lighting director, the sound director, the stage manager, technicians, etc.}

\todo{Performances}

\todo{Discussions}

The project aimed at compiling a video corpus and a fine and precise description of the rehearsals. The corpus and the descriptions rely strongly on the chosen fields of study (\todo{what's the proper translation for terrain d'études ?}), \emph{i.e.}, in our case the rehearsals of the play and the opera themselves, but more importantly the methodologies of the creative crew.

\subsection{Play and opera (experimental fields)}

Our field is based on the rehearsals of two performing shows: \emph{Chatte sur un toit brûlant}, written by Tennessee Williams and directed by Claudia Stavisky, and \emph{Elena}, composed by Francesco Cavalli, directed by Jean-Yves Ruf and conducted by Leonardo García Alarcón.
Each of these shows were rehearsed during the spring and the summer of 2013, in a period of eight and six weeks respectively. The project was about documenting the entire rehearsal process, from the first \emph{lecture à la table} where actors read the script together for the first time, to the \emph{general}, which is supposed to end the directing work and finish the rehearsals.
However, the nature of the creative work diverges considerably from the \emph{mise en scene} of a modern play to an ancient and obscure baroque opera, musically updated with a consequent adaptation work.
Those aspects motivated us to conceive a generic system that could be deployed for other contexts.

\subsection{Immersed the creative work}

A performance rehearsal is a closed-door privileged moment where actors and director work in a protective intimacy that allows them a total commitment to their art. The idea of filming those moments of intimacy, moreover exhaustively, could have been wrongly welcomed at best, or even rejected at worst, from professionals considering as sacred the privacy of rehearsal.
This aspect was taken into account in the design of the system, in order to appear the least intrusive possible towards the creative process.

\begin{figure}[htb!]
  \centering
  \includegraphics[width=\columnwidth]{elena}
  \caption{Rehearsal of \emph{Elena}, Festival d’Aix-en-Provence 2013 © Pascal Victor / ArtcomArt}
  \label{fig:elena}
\end{figure}


\subsection{Nature of rehearsals}

The rehearsals unfold according to a relatively classic \emph{modus operandi}: the acting and directing crew gathered daily in a studio room or on stage to work through certain parts of the script or of the partition. By the observation of a precedent rehearsal from the same director \emph{Mort d'un commis voyageur} \todo{reference}), we were able to identify a recurring pattern in her directing methodology: an alternation of performance, during which actors play the text/partition, and of discussion, during which the director interrupts the performance and argues with the actors. This alternation of performance and discussion proved to be central since the pattern was coded into the data model used to annotate the captured videos.
In order to precisely describe the rehearsal process, it appeared necessary to identify and label these alternations.
It is admitted that a director working according to a different methodology would have generated a very different corpus and annotation set.

%BE ce qui suit n'est pas nécessaire je pense (déjà présent section Annotation)
\todo{BE: déjà dit un peu plus loin (section Annotation)}
Usual information (\emph{i.e.} metadata) coming from the fields of theater and opera were integrated to describe the working session---with or without set, costume, lights, sound effects, music, orchestra, \emph{etc.}, but also the presence of the actors on stage and the precise text reference.
%Fin BE

\subsection{Text and partition reference}
\todo{BE: est-ce que cette sous-section est nécessaire? Quel est son contenu ?}
\todo{[...]}


%%%%%%%%%%%%%%%%
\section{The rehearsal archive: complete workflow of construction and publication process}
\todo{(2,5 pages)}

\todo{complete workflow of construction and publication process}

Digital heritage projects often focus on digitalization of analog materials and data,
when others would focus on the usage and the valorization of digitalized resources.
The specificity of our project \todo{to be homogenized: ``our''?}
was the attempt to conceive a natively digital archive from scratch
taking into account potential usages of the archive.
We therefore built a full chain from the design and the making of the archive
to its publication and usage scenarios.
This part redraws the workflow that enabled the construction and the publication of the rehearsal archive.

\subsection{Video recording}
%Non intrusive, push-button, integral, full HD, sound
\todo{BE - remarque générale : je préfère le terme annotation au terme index}
In the context described previously, the video recording required a non-intrusive and easy setup for a non-technical operator. Indeed, their profile must belong to dramaturgy and performing art, in order to dedicate their attention to performance and directing description. Therefore, the apparatus was designed to be fully ``plug\ \& play'' with a single record button to start a capture session.
The collected corpus embraces the integrality of the rehearsals, from the ``text walk-through'' at the table to the final dress rehearsal, with about five to seven working hours a day, which in consequence result in likewise five to seven hours of recorded and annotated video every day.

The unit for video recording and annotating is composed typically of a PC laptop, connected to a video camera mounted on a tripod and remotely controlled from the PC, of an omnidirectional microphone as input of a small sound mixing table, itself linked to the laptop. This system can be easily set up and wrapped up, is movable, and can be fit in a studio room without burdens. Running on a dedicated Linux distribution, the unit runs and displays only one software: an interface through which the operator can \emph{(i)} remotely control the camera and \emph{(ii)} take notes synchronously. 
The technical specifications of the original video files are the format \texttt{.mkv}, the codec H264, and the resolution of $1920 \times 1080$ pixels (full HD).

\begin{figure}[htb!]
  \centering
  \includegraphics[width=\columnwidth]{onstudio}
  \caption{Studio rehearsal of \emph{Chatte sur un toit brûlant} in Paris}
  \label{fig:onstudio}
\end{figure}

\subsection{Annotation}
\todo{attention à l'homogénéité du vocabulaire : annotations ; metadata ; data}
Simultaneously to the video capture, the operator proceeded to the exhaustive description of the rehearsals thanks to the note taking tool. The data produced during this annotation process are synchronized with the video stream and encoded into an XML format based on the Cinélab data model\footnote{Cinélab Data Model: \url{http://liris.cnrs.fr/advene/cinelab/}}.
Annotating the rehearsal relied on two main tasks: description and interpretation.  Description annotations indicated the context of the moment (performing or discussing, with or without costumes, \emph{etc.}), whereas interpretation annotations targeted the dramaturgy and the performing themselves---with free observations and comments on the creative work. The interpretations were necessarily categorized with one or several categories. Those categories emerged progressively along the rehearsal process according to the point of view and the focus of the operator. 
In contrast, different categories emerged for theater and opera, reflecting both the different nature of those performing arts, and the personal point of view of each annotator.

\begin{table}
\centering
{\small
\begin{tabular}{|l|r|r|r|}
\hline 
  & Chatte sur un toit\ldots & Elena & Total \\ 
\hline 
Number of videos & 70 & 100 & 170 \\ 
\hline 
Total duration (h) & 242 & 177 & 419 \\ 
\hline 
Total volume (TB) & 2.47 & 1.60 & 4.07 \\ 
\hline 
Number of annotations & 3,530 & 6,968 & 1,0498\\ 
\hline
\end{tabular} 
}
\caption{Facts on the video corpus}
\label{table_facts}
\end{table}

The result of this synchronous annotation was to augment the video stream with semantic metadata, also considered as intra-video index, making the video corpus easily searchable with a proper search engine.

\subsubsection{Data model} Figure~\ref{fig_data_model} describes the main elements of the data model used by the note taking tool. This model results from a workshop analysis, gathering several kinds of attendees (researchers, producers, dramatists, film editors)---each kind having different requirements concerning rehearsals annotation capabilities. It focuses on annotations that have to be created in realtime (\emph{i.e.} during the rehearsal). \newline
\todo{mise en forme}\textbf{Model description} - A session annotation is linked with a video file and represents a rehearsal session (typically lasting a half-day or a day). A session annotation is connected to a place (where the rehearsal takes place) and to a creation (linked to a work and castings). A session annotation is made up of chapters that represent performance or discussion times. A chapter contains contextual data (\emph{i.e.}, with or without costumes, lights, sets and musical inserts) and may reference concerned part of the work (text references - \emph{e.g.} the act, scene, line numbers corresponding to the beginning/ending of the section of play---\textit{service begin/end}). Chapters also may contain moments of interest (so called categories).
\begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{UMLet_Data_model}
\caption{Data model}
\label{fig_data_model}
\end{figure}
\subsubsection{Practical usage of the model} Generally speaking, at the beginning of a rehearsals caption process for a given work, an operator first fills into the note taking tool some time-stable data: the creation, work, actors, characters, rehearsals place. Some others data can next be filled or updated during a rehearsal session---\emph{e.g.}, contextual data (costumes, sets, \emph{etc.}) about a given chapter (inherited from a previous one if any).
%BE on peut pour gagner de la place juste mettre les éléments Session, Chapter, Moment of interest..

\subsubsection{Inherited categories}
\todo{BE Proposition titre subsubsection : Emerged categories review}
\todo{Emerged categories : je voulais dire, les catégories qui ont émergées lors de l'annotation. Je tente "inherited", est ce que cela convient ? Pas tout à fait clair non plus.}

Each annotation process has revealed a set of categories associated to interpretation annotations, reflecting both the related performing art and the personal point of view of the operator. The tables \ref{table:categories1} and \ref{table:categories2} present the richness of these categories and the potential of the archive for further researches in genetic analysis of creative works.

\begin{table}
\centering
\begin{tabular}{|p{4cm}|r|}
\hline 
Categories & Occurrences \\ 
\hline 
alternative performance proposition & 70 \\ 
\hline 
conflict & 7 \\ 
decision & 14 \\ 
\hline 
detecting an issue & 81 \\ 
\hline 
doubts & 23 \\ 
\hline 
improvisation & 16 \\ 
\hline 
intervention of the director & 554 \\ 
\hline 
intervention of the first assistant & 3 \\ 
\hline 
intervention of the technical staff & 22 \\ 
\hline 
opinion of an actor & 26 \\ 
\hline 
performance of an actor & 331 \\ 
\hline 
questioning & 4 \\ 
\hline 
reference to real world & 3 \\ 
\hline 
rehearsal with text & 1 \\ 
\hline 
repeating a scene & 451 \\ 
\hline 
request from an actor & 62 \\ 
\hline 
run-through & 42 \\ 
\hline 
script adaptation & 6 \\ 
\hline 
space issue & 6 \\ 
\hline 
\end{tabular}
\label{table:categories1}
\caption{Annotation categories and number of occurrences for \emph{Chatte sur un toit brulant}}
\end{table}
\todo{mise en forme des captions pour les tableaux > why capitals letter ?} 

\begin{table}
\centering
\begin{tabular}{|p{4cm}|r|}
\hline 
Categories & Occurrences \\ 
\hline 
accessories & 292 \\ 
\hline 
address & 247 \\ 
\hline 
articulation & 357 \\ 
\hline 
breathing & 25 \\ 
\hline 
character definition & 725 \\ 
\hline 
characters relation & 820 \\ 
\hline 
conflict & 27 \\ 
\hline 
cooperation director/conductor & 178 \\ 
\hline 
costumes & 711 \\ 
\hline 
decision & 154 \\ 
\hline 
decoration & 14 \\ 
\hline 
emotion & 134 \\ 
\hline 
gestures & 346 \\ 
\hline 
improvisation & 52 \\ 
\hline 
instrumentation & 118 \\ 
\hline 
intonation & 65 \\ 
\hline 
lights & 221 \\ 
\hline 
moving & 830 \\ 
\hline 
music dynamics & 158 \\ 
\hline 
partition modification & 166 \\ 
\hline 
phrasing & 131 \\ 
\hline 
power relation & 47 \\ 
\hline 
pronunciation & 109 \\ 
\hline 
reference to camera & 1 \\ 
\hline 
research & 327 \\ 
\hline 
scenery setting & 85 \\ 
\hline 
seduction relation & 66 \\ 
\hline 
tempo & 406 \\ 
\hline 
voice ``color'' & 112 \\ 
\hline 
working on the text & 104 \\ 
\hline 
\end{tabular} 
\label{table:categories2}
\caption{Annotation categories and number of occurrences for \emph{Elena}}
\end{table}  
 
\subsection{Data conformation}

\todo{(nicolas)}
\todo{@Remi: conformation est un terme anglais. Je l'explicite dans la partie ci-dessous. Je pense que cela sera compréhensible}
After the rehearsals, we centralized the data on a server for video encoding and annotation metadata confirmation. Due to the tied agenda of the project, the annotation software did not implement a strict certification on data input. Therefore, as a preliminary step before the ingestion of the corpus into the applicative platform, manual cleaning and conforming were executed on the metadata, such as merging similar categories, harmonizing the text reference format, correcting misspellings, \emph{etc.}

\subsection{Audio and video processing}
As an optional step, we use models of the actors' voices and appearances to detect and recognize them in the audio and video modalities. As explained later, this offers additional searching, browsing, and editing capabilities to our system, at the expense of (widely) increased processing time.

\subsubsection{Learning actor voices and appearances}
We use the ALIZE system for learning statistical models of each actor's voice~\cite{Bonastre05}  from one example per actor.
Each example is typically a ten-second extended speech line recorded during the first reading of the play. In parallel, we use the generative model of Gandhi and Ronfard~\cite{Gandhi13} for learning statistical models of each actor's visual appearance, using example images from at least eight viewpoints. Examples are typically extracted from the first dress rehearsal, as our appearance model is dependent on costumes.  Those models can then be used to automatically align each performance with the play script and to localize actors on stage and on screen. 

\subsubsection{Alignment between play script and performances}

Based on an off-the-shelf speaker recognition toolkit, we built a play script following algorithm where rehearsals are assumed to be generated by a probabilistic generative model.  Thus, each  line in the play script is a state in a semi-Markov model with a duration probably and a speaker voice probability. In our experiments, this methods outperformed state-of-the-art speaker diarization methods based on HMM, which do not have a duration model.
\todo{BE: Rémi, vous avez pas une réf. à vous à mettre là-dessus du coup ?}

%\begin{figure}[tp]
%\centering
% \includegraphics[width=\columnwidth]{diarizationT.png}
%\caption{In the speaker diarization phase, we learn a model of each actor's voice.
%They are used as observation probabilities in our Semi-Markov model of the rehearsals.}
%\label{fig_diarization}
%\end{figure}


\subsubsection{Actor detection and recognition} 

Based on their appearance model, we separately detect all actors present on stage. The qualitative results on this sequence are presented in Figure~\ref{fig_tracking_coahtr}. This example is particularly challenging  because of the fast displacements of the actors and the frequent occlusions between actors. 

\begin{figure}[tp]
\centering
\includegraphics[width=\columnwidth]{tracking_coahtr}
\caption{Tracking of  the six characters of sequence $\mathcal{S}_4$. Using actors-specific detections gives an additional advantage in resolving multiple actor tracking.}
\label{fig_tracking_coahtr}
\end{figure}

Despite the challenging environment, our method provides very good results for the tracking of the six actors, where generic state-of-the-art tracking methods quickly drifted away from the target and were unstable both in presence of occlusion and fast movements. 
\todo{BE: Idem : Rémi, vous avez pas une réf. à vous à mettre là-dessus du coup ?}

%\cite{Nummiaro2002,Ross2008}  \cite{Kwon2010,Babenko2009} also drifted away with heavy occlusions.  \cite{Wang2011} failed because of large appearance changes and occlusions. Only our algorithm maintains its performance in this challenging example.


\subsection{Applicative platform for data exposure}
The project built on the applicative platform \emph{Ligne de temps}\footnote{Ligne de temps: \url{http://www.iri.centrepompidou.fr/outils/lignes-de-temps/}} to implement specific modules dedicated to fine-indexed/annotated video archive of performance rehearsals. \emph{Ligne de temps} is originally a software designed for video annotation and indexation, inspired by the timelines usually used in video editing softwares, and was conceived for advanced film critique or pedagogy. It evolved into a web platform with web player and front-end annotations modules. For the purpose of the project, \emph{Ligne de temps} was adapted to embed and display annotated video of performance rehearsals, taking into consideration the particular metadata and their potential usages.

\subsubsection{Ingest}
The ingest operation was made possible thanks to the preceding data conformation. It involves automatic referencing of the video files as media contents into the platform and the synchronization of the metadata (annotations) to the media contents. \emph{Ligne de temps} provides an applicative environment to search, consult, edit and publish the media contents and their metadata, therefore brings the rehearsal annotation data from static to dynamic. \linebreak
However theater and opera archives were separated in two distinct points of access, due to usage scenarios and to differences in data format.
% BE : Difference in data format. Le format reste pour moi le même (XML ici), le modèle aussi à peu de chose près (i.e. référence à un texte pour le théâtre, référence à une partition pour l'opéra, ce qui change un peu le modèle). Par contre, le contenu des données change (i.e. les catégories, par ex.). Du coup je ne sais pas si ça vaut le coup de mettre "differences in data format", je pense que "usage scenarios" suffit...
 Indeed, the search engine and further applications require a stable model of data to offer optimized user experience.
\todo{BE remarque : supprimer cette phrase (stable model of data), cf. commentaires dans le source.}
% BE Du coup cette phrase est à supprimer de mon point de vue.

\subsubsection{Search engine}
\todo{Write}
A faceted search engine was designed to perform complex request on the archive, based on available metadata.

\begin{figure}[htb!]
  \centering
  \includegraphics[width=\columnwidth]{searchengine}
  \caption{Multi-faceted search engine for the theater archive}
  \label{fig:searchengine}
\end{figure}

The figure~\ref{fig:searchengine} displays the search interface for the theater, divided vertically in two parts for two types of search:
1) left side: searching chapters with rehearsal context data such as date of rehearsal, character(s) on stage, lines of the script, type of chapters, \emph{etc.}, providing a sortable list of chapters,
2) right side: searching interpretation annotations with full text form and/or with finished list of categories, providing a sortable list of annotations.

Thanks to the synchronization of metadata to the video stream, a search result list from either side can be used as a request to the opposite side. For instance, from the result list of annotations (right side), it is possible to extract all chapters with corresponding timecode (left side).

\subsubsection{Metadata player}
The \emph{Metadata player} is a front-end Javascript module articulating a HTML5 video player with time-stamped metadata displayed on timeline representation. It communicates with \emph{Ligne de temps} to retrieve the synchronized metadata for each media content to be displayed. For the usage of the project, a dedicated player (figure~\ref{fig:mdplayer}) was designed to display the contextual chapters and the interpretation annotations on timelines and to provide the play script that synchronized along the video, according to the text reference metadata.

\begin{figure}[htb!]
  \centering
  \includegraphics[width=\columnwidth]{mdplayer}
  \caption{the Metadata player : video player augmented with synchronized play script and metadata (annotations)}
  \label{fig:mdplayer}
\end{figure}

\subsection{Linked Open Data publication}
In order to foster and ease multiple uses of the corpus,
it is important to make the collected information as accessible as possible.
For this purpose, we chose to comply with the principles of Linked Open Data~\cite{bernerslee2006linkeddata}.
They imply that every piece of information is both identify by and accessible at a given URI, and that it is published using the RDF data model~\cite{cyganiak2014rdf11concepts}.
The Cinélab data model maps nicely into RDF, thanks to additional standards such as Media Fragments URI~\cite{troncy2012mediafragments} that are used to precisely anchor annotations to video temporal fragments. Furthermore, Linked Data has a common query language~\cite{prudhommeaux2008sparql}, so building new services on top of our corpus can be achieved with relative ease by reusing standard components. Finally, we use the approach proposed by Verborgh \emph{et al.}~\cite{verborgh2014querying} in order to make the data highly available while preserving scalability.

But the benefit of Linked Data does not only lie in standardization.
As its name implies, the key feature of this approach is to allow linking data across different sources on the Web, in order to combine it into otherwise inaccessible information.
In the context of Digital Heritage, this notion is central, as data is often scattered in multiple institutions.
While Linked Data is already widely used for museums or libraries\footnote{
See \emph{e.g.}, \url{http://data.europeana.eu/} or \url{http://data.bnf.fr/}},
its use with video has emerged more recently~\cite{vandeursen2012mediafragmentannotations,steiner2014webvtt},
and as far as we know, has not yet been widely considered for live performance archives.


%%%%%%%%%%%%%%%%
\section{Scenarios for the valorisation of the archive}
\todo{(1,5 pages)}
In parallel to the design and the implementation of the previously described workflow, the project anticipated several usage scenari of the archive, taking into consideration the potential needs of future users. A particular effort was made to address specific public with pedagogical scenari, culture mediation scenari, but also research and amateur scenari. Some of those scenari were implemented into demonstrators, others into mock-ups, allowing the partners of the project to freely explore prospective scenari according to their interest.
This prospective work was indeed central in the design of the continuous workflow, in order to articulate the newly constructed archive with existing or prospective usages. This approach is one of the particularity of this project regarding researches in the digital heritage, i.e. to have considered the archive as a living cultural object, even though the archive did not exist yet.

\subsection{Representing the archive}
The massive aspect of the archive makes is difficult to apprehend. Both the nature of the archive, i.e. video as temporal object\cite{stiegler90}\todo{ajouter référence Stiegler}, and its total duration, 419 hours for 170 video files, as well as the amount of annotations (10,498) demanded a comprehensive and cognitively acceptable representation. Taking the project's title literally: \emph{Spectacle en ligne(s)}, i.e. \emph{Performance in line(s)}, the archive can be viewed as multiple lines of rehearsals. We imagined a matrix where the abscissa was the script of the play and the ordinate was the multiple rehearsals in chronological order. The result is the distribution of the rehearsals according to the script/score over time.

\todo{Remarque BE : illustration matrice sur théâtre plutôt qu'opéra (Elena), surtout si dans un futur proche il faut se concentrer sur le théâtre pour gagner de la place.. }
\begin{figure}[htb!]
  \centering
  \includegraphics[width=0.60\columnwidth]{elenamatrix}
  \caption{Matrix of distribution of the rehearsals according to the score of \emph{Elena} over time}
  \label{fig:elenamatrix}
\end{figure}

Such matrix can be inferred from the synchronized annotations, which contains more or less precise references of the script or the score. 
\todo{Remarque BE : ci-dessous que veut-dire forcasts ? envisages plutôt non ?}
If one forcasts the paradigm with fine-grained annotations indexing precisely the script cues or the bars of the scores, the matrix could display theoretically in abscissa 1 unit for 1 cue and in ordinate 1 unit for 1 performance, i.e. a segment of rehearsal where actors are actually performing. The figure models this idealistic representation.

\todo{Remarque BE : je trouve que la figure du modèle de la matrice n'est pas super importante, à supprimer je pense si besoin de place.}
\begin{figure}[htb!]
  \centering
  \includegraphics[width=0.75\columnwidth]{fullmatrix}
  \caption{Model of the matrix of distribution of the rehearsals over time with 1 unit = 1 cue in abscissa and 1 unit = 1 performance in ordinate}
  \label{fig:fullmatrix}
\end{figure}

Such representation achieves the initial objective of providing a comprehensive visualization suitable for a mental model. In addition, it becomes a powerful visualization for browsing the archive and accessing to fine-grained annotations and video segments.

\todo{BE : pour info. j'ai unifié les deux parties d'avant : "Visualization and browsing strategies" et "Demonstrator for Brick and Maggie scene"}

\subsection{Archive access, visualization and browsing strategies}
According to usage scenarios analysis, several requirements were identified concerning the archive access, archive visualization and browsing capabilities.
In order to make the rehearsal archive accessible to the broadest possible audience, we embraced the Web platform and generated online accessible demonstrators, enabled through native HTML5 video support in all major Web browsers.\newline
Concerning archive visualization and browsing capabilities, the multiplicity of usage scenarios highlighted the fact that end users must interact with different kinds of media/content (video captations, script/score, annotations) using different manners. As a result, components for displaying/interacting with these different kinds of content were firstly implemented and can be combined to each others to form adequate hypervideos that maximize the end-user's experience. Moreover, this hypervideo design approach - based on components combinations - foster archive novel usages, as new components can be added and combined to older ones in order to fulfill new needs.\newline
Secondly, according to identified recurrent needs due to the nature of the archive, which can be seen as a set of different versions of the same object (the play or the opera), fine-grained visualisation and browsing capacities were set up to perform what we call hereafter archive temporal and spatial zooms.

\paragraph{Web Components-based approach}
We highlight that we not only optimized for reach on the consuming side---%
\emph{i.e.}, people watching the rehearsal hypervideos in their browsers---%
but also for the producing side---%
\emph{i.e.}, Web developers who collaborate with theaters and operas
to create such hypervideos in the first place and to put them online.
Web developers are used to mark up Webpages using HTML tags
like \texttt{<h1>}, \texttt{<p>}, \texttt{<strong>} \emph{etc.}
and to ``glue'' these tags functionally together using the JavaScript programming language
to finally control the look and feel of their Webpages with Cascading Stylesheets (CSS).
We were motivated to allow for the same tag-based mark-up mechanics
to also work for the creation of rich hypervideo applications.
Therefore, we have leveraged an emerging technology
that is currently being standardized by the World Wide Web Consortium (W3C)
called \emph{Web Components}.
Web Components is a~set of specifications, which let Web developers leverage
their HTML, CSS, and JavaScript knowledge to build widgets
that can be reused easily and reliably.\footnote{Web Components:
\url{http://www.chromium.org/blink/Web-components}}

\todo{BE: si besoin de place, la partie qui suit peut être supprimée}
%BE début partie pouvant être supprimée
According to a~(recently discontinued) W3C Working Draft introductory document,%
\footnote{Discontinued W3C Working Draft document:
\url{http://www.w3.org/TR/2013/WD-components-intro-20130606/}~\cite{cooney2013Webcomponents}}
the component model for the Web (``Web Components'') consists of five different pieces
listed in the following.

\begin{itemize}
  \item \textbf{Imports} which defines how templates, decorators and custom elements are packaged and loaded as a~resource%
  ~\cite{glazkov2014htmlimports}.
  \item \textbf{Shadow DOM} which encapsulates a~DOM subtree for more reliable composition of user interface elements%
  ~\cite{glazkov2014shadowdom}.    
  \item \textbf{Custom Elements} which let authors define their own elements, with new tag names and new script interfaces%
  ~\cite{glazkov2013customelements}.  
  \item \textbf{Decorators} which apply templates based on CSS selectors to affect rich visual and behavioral changes to documents.
  \item \textbf{Templates} which define chunks of inert markup that can be activated for use.  
\end{itemize}
%BE Fin partie

\noindent At time of writing, partial native support for Web Components
has landed in a~number of Web browsers,
however, for the majority of browsers,
a~so-called \emph{polyfill} solution is still required.
A~polyfill  is a~piece of code that provides the technology
that developers expect the browser to provide natively in the near future.
We rely on the Polymer project\footnote{Polymer project:
\url{http://www.polymer-project.org/}}
that provides Web Components support for many browsers.
Polymer allows us to create reusable widgets that introduce a~number of new
custom HTML elements for our task of hypervideo creation.

We have developed a~number of Web Components for the creation of hypervideos~\cite{steiner2014hypervideo}.
These Web Components are behaviorally grouped together
by a~common naming convention.
In Polymer, all element names have to start with the prefix \texttt{polymer-}
in order to create an own Web Components namespace
that distinguishes them from native HTML elements.

\begin{itemize}
  \item \texttt{<polymer-hypervideo>} is the parent element of all other elements.
    It accepts the attributes \texttt{src} for specifying a~set of
    space-separated video sources (to support different encodings),
    and---analog to the native HTML5 video attributes---%
    \texttt{width} and \texttt{height} for specifying the video's dimensions,
    then \texttt{poster} for specifying the video's poster frame, and finally \texttt{muted} to specify if the video should be initially muted.
  \item \texttt{<polymer-data-*>} is a~set of data annotation elements
    that includes the two shorthand annotation types
    \texttt{<polymer-data-actor>} for annotating video actors and
    \texttt{<polymer-data-overlay>} for annotating visual overlays,
    and the generic \texttt{<polymer-data-annotation>} for other annotations.
  \item \texttt{<polymer-track-*>} are the two textual elements
    \texttt{<polymer-track-chapters>} and \texttt{<polymer-track-subtitles>},
    which rely on WebVTT~\cite{pfeiffer2013webvtt} text tracks
    of type ``chapters'' and ``subtitles'' that they enrich~\cite{steiner2014webvtt} with
    automatically generated chapter thumbnails and a~full text view.
  \item \texttt{<polymer-visualization-*>} currently provides the
    following two visualization elements
    \texttt{<polymer-visualization-timeline>} on the one hand and 
    \texttt{<polymer-visualization-toc>} on the other
    that create a~timeline view and a~table of contents
    that put all encountered \texttt{<polymer-track-*>}
    and \texttt{<polymer-data-*>} elements in a~temporal context.
\end{itemize}

\noindent We have made an online demonstrator application available at
\url{http://spectacleenlignes.fr/hypervideo/} that showcases these Web Components
and recall that we share their implementation as open source.
A~screenshot of the application can be seen in \ref{fig:screenshot}.

\begin{figure}[htb!]
  \centering
  \includegraphics[width=0.95\linewidth]{screenshot}
  \caption{Generated hypervideo including subtitles, cue selectors, timeline, and connected play script
    showing a~rehearsal scene from day~40.}
  \label{fig:screenshot}
\end{figure}

\paragraph{Fine-grained visualization and browsing capacities}
For some selected scenes, we offer a fine-grained visualization and browsing interface allowing to zoom both temporally and spatially
through the archive by making use of the audio and video processing described in Section~4 \todo{Make sure this is correct}.  

\subsubsection{Temporal zoom} \todo{Write}

\subsubsection{Spatial zoom}
The source video  is recorded from a distant viewpoint  to provide  a general  view of the stage.  None of what occurs in the stage is missing, but the video is not pleasant to watch.  The theater performances are often edited like a movie to be more attractive.   It is easy to more precisely annotate the script of the play with some simple information as who speaks to whom. With our method, we can temporally match the script to the video.  The positions of the characters in the frames are given by our tracking algorithm.  From there, by referring to usual editing rules, we can realize an automatic editing of the performance (Figure~\ref{fig_speaker}) using methods described elsewhere~\cite{Gandhi14}.  For example, during a monologue we realize a medium-shot of the speaker and during a dialogue we realize a two-shot that contains the speaker and his addressee. This greatly improves the visibility of the main actors, and makes it possible to focus on their gestures and facial expressions, as opposed to the general view, which is more useful to appreciate the choreography of their movements on stage.

\begin{figure*}[tp]
\centering
\includegraphics[width=\textwidth]{speakers2}
\caption{Reframing of the videos at the speaker and the interlocutor levels. Audio-to-text alignment temporally localizes the cues, and the actors tracking spatially localizes the characters on the frames. 
This figure displays the automatic reframing process obtained for several frames on the sequence $\mathcal{S}_4$. The arrows go from the speaker to the interlocutor.}
\label{fig_speaker}
\end{figure*}


Future work is needed to fully evaluate this feature. We were inspired by the zoom-lens model of consciousness~\cite{Eriksen86}, 
which proposes  that human perception  may in fact compose a movie from all available visual  stimuli; and by the theory 
of spectatorship, which surveys the audience's visuomotor behavior  during a performance~\cite{Bennett97}.   

\subsection{Mobile app for the creative crew} 
\todo{Remarque BE : section à supprimer si besoin de place, mettre un petit quelque chose dans les futur work dessus}

A more complete scenario was designed to illustrate potential usage of the archive. In the context of the creative process itself, this scenario involves a mobile app that members of the crew could use to review, annotate and discuss the rehearsals along the direction process. Such an app would benefit from the complete workflow from video capture to video online publication and would provide the creative crew a excellent assistant to mark, search and archive the memory of the creative process. During the project, we developed the full user experience and interface into mock-ups (figure~\ref{fig:mobileapp}).

\begin{figure}[htb!]
  \centering
  \includegraphics[width=\columnwidth]{mobileapp}
  \caption{Mobile app : extracts of the design mock-ups}
  \label{fig:mobileapp}
\end{figure}

This scenario reveals an interesting result in terms of digital heritage issue. In fact, it appears that digital archives meet the present needs of certain users whose objectives are not necessarily to augment the production of knowledge. Digital archive should therefore articulate freely with new communities of potential actors and users. One of the results of \emph{Spectacle en ligne(s)} is the demonstration that this empowerment of users with digital ressources must be accompanied by a set of technological conditions (an applicative platform for instance). Cultural and knowledge institutions have a responsability in the design and the implementation of thoses conditions for ressources appropriation \cite{Sauret15}.

\subsection{Experimental validation}
\todo{The recordings have been evaluated subjectively by the actors, the director and her assistants?}

\todo{A separate evaluation is being performed by film editors.}

\todo{We can use Pascal Bouchez evaluation grids to assess the quality of the dataset?}


%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Limitations and Future  Work}

In this section, we describe some limitations of our approach and areas for future work.
The reframing and play text alignment is still experimental
and needs to be further validated and automated.
Therefore, better image resolution may be useful (4K, 8K)
or even stereoscopic 3D~images that can be explored in future work.
For upcoming recordings, reverse shots of the director and her assistants would be valuable.
We will take this into account for the next recording setups.
Sound quality is a problem we encountered occasionally that can be addressed with better microphones.
More work is needed to extrapolate to other performances---concerts, opera, ballet, \emph{etc.}
For the annotation tool, graphical annotations would be useful
in order to create more adequate metadata.
More future work is needed for assigning the virtual cameras automatically.
\todo{Expand more}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\todo{(0.25 pages)}

\todo{Our technology is non intrusive and well accepted by the production team}

\todo{It is made available to researchers in genetic analysis.}

 
% conference papers do not normally have an appendix

% use section* for acknowledgement
\section*{Acknowledgment}

\todo{The authors would like to thank...}

\todo{Balance reference columns}

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section
%\nocite{*}


% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{rehearsals}

% that's all folks
\end{document}
