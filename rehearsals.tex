%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[conference]{IEEEtran}
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}


\usepackage[francais,english]{babel}
\usepackage{aeguill}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage[latin1]{inputenc} %% For ISO-latin1 chars
                                %% (accented letters).
\usepackage[hyphens]{url}
\usepackage[pdftex,urlcolor=black,colorlinks=true,linkcolor=black,citecolor=black]{hyperref}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Subsection}

\usepackage{txfonts}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )

% todo macro
\usepackage{color}
\newcommand{\todo}[1]{\noindent\textcolor{red}{{\bf \{ToDo} #1{\bf \}}}}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor ALIZE}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Video Recording and Indexing of Theater Rehearsals}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
%\author{
%\IEEEauthorblockN{Rémi Ronfard\\ Vineet Gandhi}
%\IEEEauthorblockA{INRIA, LJK\\
%University of Grenoble, France\\
%Email: remi.ronfard@inria.fr}
%\and
%\IEEEauthorblockN{Benoit Encelle\\ Pierre-Antoine Champin\\ Thomas Steiner}
%\IEEEauthorblockA{Université Claude Bernard Lyon~1, France\\
%Email: benoit.encelle@univ-lyon1.fr\\
%Email: \{pierre-antoine.champin, tsteiner\}@liris.cnrs.fr}
%\and
%\IEEEauthorblockN{Nicolas Sauret}
%\IEEEauthorblockA{IRI, France\\
%Email: nicolas.sauret@iri.centrepompidou.fr}
%\and
%\IEEEauthorblockN{Cyrille Migniot}
%\IEEEauthorblockA{University of Burgundy, France\\
%Email: xxx}
%}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
\author{\IEEEauthorblockN{Rémi Ronfard,\IEEEauthorrefmark{1} Vincet Gandhi,\IEEEauthorrefmark{1}
Benoit Encelle,\IEEEauthorrefmark{2} P.-A. Champin,\IEEEauthorrefmark{2} Thomas Steiner\IEEEauthorrefmark{2}
Nicolas Sauret,\IEEEauthorrefmark{3} 
Cyrille Migniot\IEEEauthorrefmark{4}}
\IEEEauthorblockA{\IEEEauthorrefmark{1} INRIA, LJK, University of Grenoble, France.
Email: remi.ronfard@inria.fr}
\IEEEauthorblockA{\IEEEauthorrefmark{2} Université Claude Bernard Lyon~1, France.\\
Email: benoit.encelle@univ-lyon1.fr, \{pierre-antoine.champin, tsteiner\}@liris.cnrs.fr}
\IEEEauthorblockA{\IEEEauthorrefmark{3} IRI, France.
Email: nicolas.sauret@iri.centrepompidou.fr}
\IEEEauthorblockA{\IEEEauthorrefmark{4} University of Burgundy, France.
Email: xxx}
}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}
%\boldmath
\todo{Write abstract}
\end{abstract}
% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the conference you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals/conferences frown on
% math in the abstract anyway.I. INTRODUCTION (1 page)

% no keywords

\begin{keywords}
\todo{Write keywords}
\end{keywords}

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



%%%%%%%%%%%%%%%%
\section{Introduction}

\todo{0.5 pages (Rémi)}

Within the world of theater the rehearsal room is a sacred space---the private domain where boundaries are pushed, risks are taken, mistakes made, vulnerabilities exposed and, at its very best, magic created. It is not a place into which the public is often, if ever, invited. Until now. (reproduced from the liner notes to ``in the company of actors''). This paper describes the result of a two-year project dedicated to the creation of a digital archive  of  the complete  rehearsals of a theater production at theater de Lyon Célestins.

\todo{Spectacle en Ligne(s): complete workflow for a natively digital archive from capture to editorialization
(captation/annotation/enrichissement/publication/éditorialisation)}

In 1966, Jean-Luc Godard remarked: {\em ``Why do theater people never film their performances to keep  them as an archive? This would be very simple: put the camera in the middle of the orchestra seats with a medium lens---not a zoom lens, because it would already be making choices and propose an interpretation''}~\cite{Godard66}.

The objective of the \emph{Spectacle en Ligne(s)} research project was to develop novel intuitive and interactive software tools to make it easier to 
create and publish annotated video archives of theater performance and rehearsals. As a case study, the project is releasing an archive of 
the complete rehearsals of a French translation and adaptation of \emph{Cat on a hot tin roof} by Tennessee Williams at the theater de Lyon Célestins
in 2012. 

The French title of the project refers to the multiple timelines at work in this archive---the timeline of the production from the first readings to the opening night, and the storyline of the play. The title also refers to the important problems associated with putting such an archive ``online'' on the theater's website.

\todo{Paragraph on legal aspects?} 

\todo{Paragraph on motivations and audiences?} 

\todo{Paragraph on technical achievements of the project?}

The paper is organized as follows. Section~1 reviews previous work in using video as an archive for theater. Section~2 describes
context of theater rehearsals and introduces the key concepts that govern the archive. Section~3 describes the workflow that was
created to produce, annotate, and publish the archive. Section~4 describes usage scenarios and applications that were developed
to support access to the archive by different audiences. Section~5 reviews limitations and future work.
\todo{Make sure the paper structure is correctly described}


%%%%%%%%%%%%%%%%
\section{State of the art}
\todo{State of the art in theater, computers and video (1 page - Rémi)}

\todo{Why record rehearsals?}
\todo{Why index them?}
\todo{Who are the targeted audiences? Professors, researchers, students, and amateurs?}
\todo{We can use Pascal Bouchez evaluation grids to assess the quality of the dataset?}
\todo{Our common goal is to show theater at work?}

\todo{Not much work dedicated to document the rehearsal process.}

\todo{Not much work dedicated to document the mise-en-scene.}

\todo{Mention the Brecht archives, the Stanislavsky archives.}

\todo{[Regarder l'annexe technique du projet]}
\todo{[+ présentation REMI à la FFIRT?]}

Archiving the recordings of theater performances is important for preserving  cultural heritage. 
Yet, it is not yet a common or systematic practice, and theater remains known as {\em the ephemeral art}\todo{cite}.
This is likely to change in the near future, as high-quality video recordings are becoming more widely
available to theaters.   Many organizations around the world now actively produce and
archive video recordings of theater performances. The theater on Film and Tape Archive
(TOFT) provides research access to video recordings of New York and regional theater productions
since 1970 and the collection now amounts to 7,469 titles with a total of 20,000 items. 
The National Video Archive of Performance (NVAP) at Victoria and Albert Museum London
provides research access to video recordings of numerous renowned theater performances produced
across England since 1992. 

Another relevant case study  is the successful ventures providing recorded
theater performances on the Web. Recently launched companies like Digital Theater use their
own equipment to record theater productions, then sell them for far less than the cost of an
actual theater ticket to watch online, but from the best spot possible. The Website \todo{add footnote} of the French
National Institute of Audiovisual (INA) also allows viewers to download recorded theater plays
at considerably low prices. You can watch the trailers of theater performances just like movies
on a variety of devices and then download the entire play if it is of your interest. The Web platforms
have also allowed for free form experimentation with recorded sequences, which is not
so convenient on actual TV or cinema. For example, FranceTV channel in 2013 created a Website
\todo{add footnote} showing five different versions of the same play \emph{theater sans animaux}, showing both
how a theater play develops in different stages and how it can be presented to the audience in
different styles. The Web platform has also allowed several theaters to share the extracts and
trailers of upcoming performances for publicity purposes. Importantly, Web based platforms
allow even the small production companies or individuals to easily share the recorded content
with the desired audience.

In this paper, we describe our effort to create video recordings of the complete rehearsals,
rather than a single performance, of a theater play and publishing it online. This raises original
and interesting scientific, technological, and social questions.

It is a difficult task to create recorded performances to match up to the live experience
of watching a play. When you watch a stage play, you generally only get to observe the actions 
of the performers from one static viewpoint---your seat. But if one was to watch a recorded 
performance from a static viewpoint covering the entire action, it could become monotonous 
to hold the interest of the viewer. Recordings from a single static viewpoint may also fail to 
highlight important details in the scene. Even with these limitations and inability to create a nice 
viewing experience, this approach is still commonly practiced in small scale productions due to 
its convenience, requiring just a single static camera covering the entire stage. 


On the other hand, television broadcasts of theater typically use a large variety of viewpoints, which are edited together
in post-production or live~\cite{bouchez2007filmer}. One extreme example is John Gielgud's 
\emph{Hamlet} starring Richard Burton in 1964, which used  the now defunct Electronovision system of 19 cameras. 
One problem with the ``multiple camera, multiple takes'' technique used by television broadcasters 
is that it is costly and time-consuming to produce; and it generates huge volumes of data, which are 
difficult to organize into an accessible and usable archive. Or more frequently, they are just ignored
and only the edited version is kept as an archive. Which brings us back to Godard's citation. How can we create 
an archive which is  more than an interpretation made by a television director and which is more than a video surveillance recording of the theater stage? 

Our approach in this paper is to make video recordings from a single viewpoint (the director's seat) with the highest possible
video resolution and to augment them with detailed, high-level  annotations that make it possible to quickly  search, browse
and even re-edit the archive interactively. 


\todo{Show image of the camera and room?}

What we lose is the variety of viewpoints, which is typical for television broadcasts. What we gain is the ability to compare
different versions, measure progress, and understand the rehearsal process by the virtue of having a single spatial reference.
We also experimented with novel techniques for varying the framings and shot sizes interactively 
by making use of a {\em virtual}  zoom lens~\cite{Gandhi14}, which makes it possible for the audience to
use the archive as {\em stock footage} which they can re-frame and re-edit according to their own interpretation (this is described in Section 5) \todo{Make sure this is correct}. 


\paragraph*{Video and theater}

The analysis of performance requires a working knowledge of semiotics, reception studies and theories 
of ideology, gender, corporeality, space and ethnicity in addition to experience of the art form in question~\cite{Auslander97,Counsell01}.

Audio and video recordings are useful resources for studying audio and visual interaction between actors~\cite{Fitzpatrick90}
and of course in the field of rehearsal studies~\cite{McAuley98a,McAuley98b,McAuley06,McAuley08}.

Yet, in practice, researchers have limited access to performances and must often refer to their own memories of the production or the rehearsals~\cite{Selbourne82,Sher85,Stafford00,Stern00}. A notable exception is {\em In the company of actors}, a documentary featuring an ensemble of Australian actors, as they prepare to perform the Sydney theater Company's production of \emph{Hedda Gabler}, with Cate Blanchett in the title role at the prestigious Brooklyn Academy of Music  in New York~\cite{Darling07}. But even in that case, only an edited version (an interpretation)  of the rehearsals is made available to the public. 

\todo{Other exceptions: Ariane Mnouchkine, John Gelgud.}

\paragraph*{Audio and video processing}
A useful feature for a large video archive is the capability to retrieve video segments based on which actors 
are performing and which segment of the play script they are performing. The topics of actor~\cite{Hilton06} and speaker~\cite{Miro12} recognition are very active in the audio and video processing research communities, but very little academic 
work has been dedicated to the special case of theater. The theater stage is a complex and cluttered environment 
with complex lighting and vast dimensions, which makes automatic audio and video processing quite challenging. 
Swedbeerg describes a system for localizing actors on a stage using RFID sensors carried by the actors~\cite{Swedberg}
but their system is intrusive and costly, which severely limits its usefulness in practice.

A common approach in computer vision for detecting and recognizing people uses generic people detectors~\cite{Ronfard02,
Dalal05,Felzenszwalb10,Andriluka12} followed by re-identification~\cite{TapaswiBS12}. Gandhi and Ronfard have evaluated 
that approach in the special cases of detecting and naming actors in films and on stage~\cite{Gandhi13} and found that such 
approaches miss the actors in 40\% of the cases on average. As an alternative, they have proposed
methods for separately learning appearance models of actors and detecting them in parallel, and showed that the number
of missed actors (false negatives) drops to 20\% on average. We used their method in \emph{Spectacle en Ligne(s)} for localizing
actors in selected scenes of the archive.

One approach to indexing of theater rehearsals and performances is to automatically align
them to the play script. The {\em script-sync} feature in Avid Media Composer,
an off-the-shelf tool, allows for the alignment of video recordings with their scripts word-by-word. Such systems
are based on Hidden Markov Models (HMM) and work by computing  a forced alignment 
between the play script and the audio recording. Nevertheless they require a very high quality 
sound recording and fail in the presence of music, reverberation and background noise. This makes them
unsuitable for our case. Speaker change detection (also called speaker diarization) is an alternative method
which provides line-by-line, rather than word-by-word alignments and has been demonstrated 
on television shows~\cite{Sankar09} and theater recordings~\cite{Caillet07,Caillet13}. In Section~5 \todo{Make sure this is correct},
we describe a special-purpose speaker diarization method that we have developed for aligning multiple versions 
of the same scene temporally using the common play script as a pivot representation.


\paragraph*{Hypervideo and interactive documentaries}

The term \emph{hypervideo} is commonly used to refer to
\textit{``a~displayed video stream that contains embedded user-clickable anchors''}%
~\cite{sawhney1996hypercafe,smith2002extensible}
and annotations, allowing for navigation between the video and other hypermedia elements.
In a~2006 article in \emph{The Economist}, the authors write 
\textit{``[h]yperlinking video involves the use of `object-tracking' software
to make filmed objects, such as cars, clickable as they move around.
Viewers can then click on items of interest in a~video
to watch a~related clip; after it has played,
the original video resumes where it left off.
To inform viewers that a~video is hyperlinked,
editors can add highlights to moving images, use beeps as audible cues,
or display still images from hyperlinked videos
next to the clip that is currently playing''}~\cite{economist2006hypervideo}.
In standard literature, hypervideo is considered a~logical consequence
of the related concept of \emph{hypertext}~\cite{bernerslee1990hypertext}.
In contrast to hypertext, hypervideo necessarily includes a~time component,
as content changes over time.
Therefore hypervideo has other technical and aesthetic requirements
than hypertext, the most obvious one being appropriate segmentation in scenes
or even objects.
The opportunities for feature-rich semantic hypervideos are endless,
only limited by feasibility and ease of their creation.
In this paper, we share our approach to affordably and practically document
the creation of theater and opera productions with video and Web technologies.


%%%%%%%%%%%%%%%%
\section{The rehearsal process}

\todo{Contexte de l'archive (1 page - Nicolas)}

\todo{du terrain à la donnée: ce qui nous a amené à structurer les données de cette façon}

\todo{description des besoins}

\todo{Actors and participants:  theater rehearsals involve many actors and participants, not limited to actors in the play.}

\todo{Other participants include the director and her assistants, the lighting director, the sound director,
the stage manager, technicians, etc.}

\todo{Performances}

\todo{Discussions}

The project aimed at compiling a video corpus and a fine and precise description of the rehearsals. The corpus and the descriptions relies strongly on the chosen fields of study (\emph{terrain d'études}), \emph{i.e.}, in our case the play and the opera themselves, but more importantly the methodologies of the creative crew.

\subsection{Play and opera (experimental fields)}

Our field is based on the rehearsals of two performing shows: \emph{Chatte sur un toit brûlant} \todo{reference} and \emph{Elena} \todo{reference}.
Each of these shows were rehearsed during the spring and the summer of 2013, in a period of six and seven weeks \todo{check exact duration} respectively. The project was about documenting the entire rehearsal process, from the first \emph{lecture à la table} where actors read the script together for the first time, to the \emph{general}, which is supposed to end the directing work and finish the rehearsals.
However, the nature of the creative work diverges considerably from the \emph{mise en scene} of a modern play to an ancient and obscure baroque opera, musically updated with a consequent adaptation work.
Those aspects motivated us to conceive a generic system that could be deployed for other contexts.

\subsection{Immersed the creative work}

A performance rehearsal is a closed-door privileged moment where actors and director work in a protective intimacy that allows them a total commitment to their art. The idea of filming those moments of intimacy, moreover exhaustively, could have been wrongly welcomed at best, or even rejected at worst, from professionals considering as sacred the privacy of rehearsal.
This aspect was taken into account in the design of the system, in order to appear the least intrusive possible towards the creative process.

\subsection{Nature of rehearsals}

The rehearsals unfold according to a relatively classic \emph{modus operandi}: the acting and directing crew gathered daily in a studio room or on stage to work through certain parts of the script or of the partition. By the observation of a precedent rehearsal from the same director \emph{Mort d'un commis voyageur} \todo{reference}), we were able to identify a recurring pattern in her directing methodology: an alternation of performance, during which actors play the text/partition, and of discussion, during which the director interrupts the performance and argues with the actors. This alternation of performance and discussion proved to be central since the pattern was coded into the indexation data model of the captured videos.
In order to precisely describe the rehearsal process, it appeared necessary to identify and label this alternation.
It is admitted that a director working according to a different methodology would have generated a very different corpus and index.
Usual categories coming from the fields of theater and opera were integrated to describe the working session---with or without set, costume, lights, sound effects, music, orchestra, \emph{etc.}, but also the presence of the actors on stage and the precise text reference.

\subsection{Text and partition reference}

\todo{[...]}



%%%%%%%%%%%%%%%%
\section{The rehearsal archive : complete workflow of construction and publication process}
\todo{(2,5 pages)}

\todo{complete workflow of construction and publication process}

Digital heritage projects often focus on digitalization of analogical materials and datas, when other would focus on the usage and the valorisation of digitalized ressources.
The specificity of our project \todo{to be homogeneised: "our" ?} was the attempt to conceive a natively digital archive from scratch taking into account potential usages of the archive. We built therefore a full chain from the design and the making of the archive to its publication and usage scenarios.
This part redraws the workflow that enabled the construction and the publication of the video archive.

\subsection{Video recording}
%Non intrusive, push-button, integral, full HD, sound
In the context described previously, the video recording required a non-intrusive and easy setup for a non-technical operator. Indeed, their profile must belong to dramaturgy and performing art, in order to dedicate their attention to performance and directing description. Therefore, the apparatus was designed to be fully ``plug\ \& play'' with a single record button to start a capture session.
The collected corpus embraces the integrality of the rehearsals, from the ``text walk-through'' at the table to the final dress rehearsal, with about five to seven working hours a day, which in consequence result in likewise five to seven hours of recorded and indexed video every day.
The unit for video recording and indexing is composed typically of a PC laptop, connected to a video camera mounted on a tripod and remotely controlled from the PC, of an omnidirectional microphone as input of a small sound mixing table, itself linked to the laptop. This system can be easily set up and wrapped up, is movable, and can be fit in a studio room without burdens. Running on a dedicated Linux distribution, the unit runs and displays only one software: an interface through which the operator can \emph{(i)} remotely control the camera and \emph{(ii)} take notes synchronously. 
The technical specifications of the original video files are the format \texttt{.mkv}, the codec H264, and the resolution of $1920 \times 1080$ pixels (full HD).

\subsection{Annotation}

Simultaneously to the video capture, the operator proceeded to the exhaustive description of the rehearsals thanks to the note taking tool. The data produced during this annotation process is encoded into an XML format according to the data model Cinélab \todo{add citation} described in more detail further on in the text, and which guarantees the synchronization of the annotations with the video stream. 
Annotating the rehearsal relied on two main tasks: \emph{(i)} description and \emph{(ii)} interpretation. The description consisted of indexing the context of the moment (performing or discussing, text lines, with or without costumes, \emph{etc.}), whereas the interpretations targeted the dramaturgy and the performing themselves, with free observations and comments on the creative work. The interpretations were necessarily categorized with one or several categories. Those categories emerged progressively along the rehearsal process according to the point of view and the focus of the operator. 
In contrast, different categories emerged for theater and opera, reflecting both the different nature of those performing arts, and the personal point of view of each annotator.

The result of this synchronous annotation was to augment the video stream with semantic metadata, also considered as intra-video index, making the video corpus easily searchable with a proper search engine.

\subsubsection{live performed}

\todo{(nicolas)}

\subsubsection{Data models}
\todo{BE reprise élément livrable modèle de connaissance pour les répétitions théâtrales (diag. UML+description), cf. livrable}
Figure X \todo{add figure} describes the annotation model used by the note taking tool. This model results from a workshop analysis, gathering several kinds of attendees (researchers, producers, dramatists, film editors)---each kind having different requirements concerning rehearsals annotation.\todo{BE footnote avec lien vers workshop ? (en francais...)}. This model focused on annotations that have to be created in realtime, \emph{i.e. }, during the rehearsal.
A session annotation is linked with a video file and represents a rehearsal session (typically lasting a half-day or a day). A session annotation is connected to a place (where the rehearsal takes place) and to a creation. A session annotation is made up of chapters that represent performance or discussion times. A chapter contain contextual data about this performance/discussion time (\emph{i.e.}, with or without costumes, lights, sets, and musical inserts) and references concerned part of the work.



\subsection{Data conformation}

\todo{(nicolas)}

\todo{Problem and solution? Manually done with internal tools}

\subsection{Audio and video processing}
As an optional step, we use models of  the actors' voices and appearances to detect and recognize them
in the audio and video modalities. As explained later, this offers additional searching, browsing, and editing capabilities
to our system, at the expense of (widely) increased processing time.

\subsubsection{Learning actor  voices and appearances}
We use the ALIZE system for learning statistical models of each actor's voice~\cite{Bonastre05}  from one example per actor.
Each example is typically a ten-second extended speech line recorded during the first reading of the play. In parallel,
we use the generative model of Gandhi and Ronfard~\cite{Gandhi13} for learning statistical models of each actor's visual appearance, 
using example images from at least eight viewpoints. Examples are typically extracted from the first dress rehearsal,
as our appearance model is dependent on costumes.  Those models can then be used to automatically align
each performance with the play script and to localize actors on stage and on screen. 

\subsubsection{Alignment between play script and performances}

Based on an off-the-shelf speaker recognition toolkit, we built a play script following algorithm where rehearsals 
are assumed to be generated by a probabilistic generative model.  Thus, each  line in the play script is a state 
in a semi-Markov model with a duration probably and a speaker voice probability. In our experiments, this methods
outperformed state-of-the-art speaker diarization methods based on HMM, which do not have a duration model.

%\begin{figure}[tp]
%\centering
% \includegraphics[width=\columnwidth]{diarizationT.png}
%\caption{In the speaker diarization phase, we learn a model of each actor's voice.
%They are used as observation probabilities in our Semi-Markov model of the rehearsals.}
%\label{fig_diarization}
%\end{figure}


\subsubsection{Actor detection and recognition} 

Based on their appearance model, we separately detect all actors present on stage. The qualitative results on this sequence 
are presented in Figure~\ref{fig_tracking_coahtr}. This example is particularly challenging  because of the fast displacements 
of the actors and the frequent occlusions between actors. 

\begin{figure}[tp]
\centering
\includegraphics[width=\columnwidth]{tracking_coahtr}
\caption{Tracking of  the six characters of sequence $\mathcal{S}_4$. Using actors-specific detections gives 
an additional advantage in resolving multiple actor tracking.}
\label{fig_tracking_coahtr}
\end{figure}

Despite the challenging environment, our method provides very good results for the tracking of the six actors, where generic state-of-the-art tracking methods quickly drifted away from the target  and were unstable both in presence of occlusion and fast movements. 


%\cite{Nummiaro2002,Ross2008}  \cite{Kwon2010,Babenko2009} also drifted away with heavy occlusions.  \cite{Wang2011} failed because of large appearance changes and occlusions. Only our algorithm maintains its performance in this challenging example.



\subsection{Applicative plateforme for data exposure}

\todo{Ligne de temps (nicolas , PA)}

\subsubsection{Ingest}
\todo{Write}

\subsubsection{Search engine}
\todo{Write}

\subsubsection{Metadata player}
\todo{Write}

\subsection{Linked Open Data  publication}
In order to foster and ease multiple uses of the corpus,
it is important to make the collected information as accessible as possible.
For this purpose,
we chose to comply with the principles of Linked Open Data~\cite{bernerslee2006linkeddata}.
They imply that every piece of information is both identify by and accessible at a given URI,
and that it is published using the RDF data model~\cite{cyganiak2014rdf11concepts}.
The Cinélab data model maps nicely into RDF,
thanks to additional standards such as Media Fragment URIs~\cite{troncy2012mediafragments}
that are used to precisely anchor annotations to video temporal fragments.
Furthermore, Linked Data has a common query language~\cite{prudhommeaux2008sparql}
so building new services on top of our corpus can be achieved with relative ease by reusing standard components.
Finally, we use the approach proposed by Verborgh \textit{et al.}~\cite{verborgh2014querying}
in order to make the data highly available while preserving scalability.

But the benefit of Linked Data does not only lie in standardization.
As it name implies,
the key feature of this approach is to allow linking data across different sources on the Web,
in order to combine it into otherwise inaccessible information.
In the context of Digital Heritage,
this notion is central, as data is often scattered in multiple institutions.
While Linked Data is already widely used for museums or libraries\footnote{
See \textit{i.e.} \url{http://data.europeana.eu/} or \url{http://data.bnf.fr/}},
its use with video has emmerged more recently~\cite{vandeursen2012mediafragmentannotations,steiner2014webvtt},
and as far as we know,
has not yet been widely considered for live performance archives.




%%%%%%%%%%%%%%%%
\section{Archive Valorisation Scenarios}
\todo{(1,5 pages)}

\subsection{Visualization and browsing strategies}

In order to make the video archive accessible to the broadest possible audience,
we decided to embrace the Web platform and generated online accessible demonstrators
with interactive hypervideo experiences,
enabled through native HTML5 video support in all major Web browsers.

We highlight that we not only optimized for reach on the consuming side---%
\emph{i.e.}, people watching the rehearsal hypervideos in their browsers---%
but also for the producing side---%
\emph{i.e.}, Web developers who collaborate with theaters and operas
to create such hypervideos in the first place and to put them online.
Web developers are used to mark up Webpages using HTML tags
like \texttt{<h1>}, \texttt{<p>}, \texttt{<strong>} \emph{etc.}
and to ``glue'' these tags functionally together using the JavaScript programming language
to finally control the look and feel of their Webpages with Cascading Stylesheets (CSS).
We were motivated to allow for the same tag-based mark-up mechanics
to also work for the creation of rich hypervideo applications.
Therefore, we have leveraged an emerging technology
that is currently being standardized by the World Wide Web Consortium (W3C)
called \emph{Web Components}.
Web Components is a~set of specifications, which let Web developers leverage
their HTML, CSS, and JavaScript knowledge to build widgets
that can be reused easily and reliably.\footnote{Web Components:
\url{http://www.chromium.org/blink/Web-components}}
According to a~(recently discontinued) W3C Working Draft introductory document,%
\footnote{Discontinued W3C Working Draft document:
\url{http://www.w3.org/TR/2013/WD-components-intro-20130606/}~\cite{cooney2013Webcomponents}}
the component model for the Web (``Web Components'') consists of five different pieces
listed in the following.

\begin{itemize}
  \item \textbf{Imports} which defines how templates, decorators and custom elements are packaged and loaded as a~resource%
  ~\cite{glazkov2014htmlimports}.
  \item \textbf{Shadow DOM} which encapsulates a~DOM subtree for more reliable composition of user interface elements%
  ~\cite{glazkov2014shadowdom}.    
  \item \textbf{Custom Elements} which let authors define their own elements, with new tag names and new script interfaces%
  ~\cite{glazkov2013customelements}.  
  \item \textbf{Decorators} which apply templates based on CSS selectors to affect rich visual and behavioral changes to documents.
  \item \textbf{Templates} which define chunks of inert markup that can be activated for use.  
\end{itemize}

\noindent At time of writing, partial native support for Web Components
has landed in a~number of Web browsers,
however, for the majority of browsers,
a~so-called \emph{polyfill} solution is still required.
A~polyfill  is a~piece of code that provides the technology
that developers expect the browser to provide natively in the near future.
We rely on the Polymer project\footnote{Polymer project:
\url{http://www.polymer-project.org/}}
that provides Web Components support for many browsers.
Polymer allows us to create reusable widgets that introduce a~number of new
custom HTML elements for our task of hypervideo creation.

We have developed a~number of Web Components for the creation of hypervideos~\cite{steiner2014hypervideo}.
These Web Components are behaviorally grouped together
by a~common naming convention.
In Polymer, all element names have to start with the prefix \texttt{polymer-}
in order to create an own Web Components namespace
that distinguishes them from native HTML elements.

\begin{itemize}
  \item \texttt{<polymer-hypervideo>} is the parent element of all other elements.
    It accepts the attributes \texttt{src} for specifying a~set of
    space-separated video sources (to support different encodings),
    and---analog to the native HTML5 video attributes---%
    \texttt{width} and \texttt{height} for specifying the video's dimensions,
    then \texttt{poster} for specifying the video's poster frame, and finally \texttt{muted} to specify if the video should be initially muted.
  \item \texttt{<polymer-data-*>} is a~set of data annotation elements
    that includes the two shorthand annotation types
    \texttt{<polymer-data-actor>} for annotating video actors and
    \texttt{<polymer-data-overlay>} for annotating visual overlays,
    and the generic \texttt{<polymer-data-annotation>} for other annotations.
  \item \texttt{<polymer-track-*>} are the two textual elements
    \texttt{<polymer-track-chapters>} and \texttt{<polymer-track-subtitles>},
    which rely on WebVTT~\cite{pfeiffer2013webvtt} text tracks
    of type ``chapters'' and ``subtitles'' that they enrich~\cite{steiner2014webvtt} with
    automatically generated chapter thumbnails and a~full text view.
  \item \texttt{<polymer-visualization-*>} currently provides the
    following two visualization elements
    \texttt{<polymer-visualization-timeline>} on the one hand and 
    \texttt{<polymer-visualization-toc>} on the other
    that create a~timeline view and a~table of contents
    that put all encountered \texttt{<polymer-track-*>}
    and \texttt{<polymer-data-*>} elements in a~temporal context.
\end{itemize}

\noindent We have made an online demonstrator application available at
\url{http://spectacleenlignes.fr/hypervideo/} that showcases these Web Components
and recall that we share their implementation as open source.
A~screenshot of the application can be seen in \autoref{fig:screenshot}.

\begin{figure}[htb!]
  \centering
  \includegraphics[width=0.95\linewidth]{screenshot}
  \caption{Generated hypervideo including subtitles, cue selectors, timeline, and connected play script
    showing a~rehearsal scene from day~40.}
  \label{fig:screenshot}
\end{figure}

\subsection{Demonstrator for Brick and Maggie scene}
For some selected scenes, we offer a fine-grained browsing interface allowing to zoom both temporally and spatially
through the archive by making use of the audio and video processing described in Section~4 \todo{Make sure this is correct}.  

\subsubsection{Temporal zoom} \todo{Write}

\subsubsection{Spatial zoom}
The source video  is recorded from a distant viewpoint  to provide  a general  view of the stage.  None of what occurs in the stage is missing, but the video is not pleasant to watch.  The theater performances are often edited like a movie to be more attractive.   It is easy to more precisely annotate the script of the play with some simple information as who speaks to whom. With our method, we can temporally match the script to the video.  The positions of the characters in the frames are given by our tracking algorithm.  From there, by referring to usual editing rules, we can realize an automatic editing of the performance (Figure~\ref{fig_speaker}) using methods described elsewhere~\cite{Gandhi14}.  For example, during a monologue we realize a medium-shot of the speaker and during a dialogue we realize a two-shot that contains the speaker and his addressee. This greatly improves the visibility of the main actors, and makes it possible to focus on their gestures and facial expressions, as opposed to the general view, which is more useful to appreciate the choreography of their movements on stage.

\begin{figure*}[tp]
\centering
\includegraphics[width=\textwidth]{speakers2}
\caption{Reframing of the videos at the speaker and the interlocutor levels. Audio-to-text alignment temporally localizes the cues, and the actors tracking spatially localizes the characters on the frames. 
This figure displays the automatic reframing process obtained for several frames on the sequence $\mathcal{S}_4$. The arrows go from the speaker to the interlocutor.}
\label{fig_speaker}
\end{figure*}


Future work is needed to fully evaluate this feature. We were inspired by the zoom-lens model of consciousness~\cite{Eriksen86}, 
which proposes  that human perception  may in fact compose a movie from all available visual  stimuli; and by the theory 
of spectatorship, which surveys the audience's visuo-motor behavior  during a performance~\cite{Bennett97}.   





\subsection{Mobile app} 

\todo{If space permits!}

\todo{Maquettes?}

\todo{Not sure that this is relevant here!}

\subsection{Experimental validation}
\todo{The recordings have been evaluated subjectively by the actors, the director and her assistants?}

\todo{A separate evaluation is being performed by film editors.}

\todo{We can use Pascal Bouchez evaluation grids to assess the quality of the dataset?}


%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Limitations and Future  Work}

In this section, we describe some limitations of our approach and areas for future work.
The reframing and play text alignment is still experimental
and needs to be further validated and automated.
Therefore, better image resolution may be useful (4K, 8K)
or even stereoscopic 3D~images that can be explored in future work.
For upcoming recordings, reverse shots of the director and her assistants would be valuable.
We will take this into account for the next recording setups.
Sound quality is a problem we encountered occasionally that can be addressed with better microphones.
More work is needed to extrapolate to other performances---concerts, opera, ballet, \emph{etc.}
For the annotation tool, graphical annotations would be useful
in order to create more adequate metadata.
More future work is needed for assigning the virtual cameras automatically.
\todo{Expand more}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\todo{(0.25 pages)}

\todo{Our technology is non intrusive and well accepted by the production team}

\todo{It is made available to researchers in genetic analysis.}

 
% conference papers do not normally have an appendix

% use section* for acknowledgement
\section*{Acknowledgment}

\todo{The authors would like to thank...}

\todo{Balance reference columns}

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section
%\nocite{*}


% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{rehearsals}



% that's all folks
\end{document}


